{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lM3uQL-0jv-G"
   },
   "source": [
    "### Libraries Required\n",
    "\n",
    "libraries for dataset preparation, feature engineering, model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "wKdRf87uj9Xg",
    "outputId": "37c4c3b2-f3e8-4c06-e3f0-058f212bc7b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/My Drive/Deliverable\n"
     ]
    }
   ],
   "source": [
    "# if you want to run it in colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/My Drive/Deliverable\n",
    "\n",
    "# !pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "z5Fin8Tejv-J",
    "outputId": "d6f8c03d-7027-46ed-c197-07816c072677"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import keras\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection ###use for train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder # Use sklearn utility to convert label strings to numbered index\n",
    "from keras.preprocessing import text, sequence ## for text preprocessing\n",
    "from keras import layers, models, optimizers ##keras model making function\n",
    "from keras.utils import to_categorical #for preprocessing\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "47dJqbGhjv-N"
   },
   "source": [
    "#### Libraries Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82
    },
    "colab_type": "code",
    "id": "kP2R1slIjv-O",
    "outputId": "e4613fee-b878-4b87-c278-d0b476865516"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy tested version:  1.16.2\n",
      "pandas tested version:  0.24.2\n",
      "keras tested version:  2.2.4\n",
      "sklearn tested version:  0.21.1\n"
     ]
    }
   ],
   "source": [
    "print ('Numpy tested version: ',np.__version__)\n",
    "print ('pandas tested version: ',pd.__version__)\n",
    "print ('keras tested version: ',keras.__version__)\n",
    "print ('sklearn tested version: ',sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D3Zacai2jv-U"
   },
   "source": [
    "## Dataset preparation\n",
    "\n",
    "1. Loading the dataset\n",
    "2. Dataset Features Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "3def70f0527177e1e6fdfd024b79804402e9c088",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "PBJ8CTYL7vZD",
    "outputId": "92f52c7e-3ef1-4fda-b990-52af0cef26b9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>politics</td>\n",
       "      <td>From: thomasr@cpqhou.se.hou.compaq.com (G. Tho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>politics</td>\n",
       "      <td>ukip s secret weapon  by any measure  new york...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>politics</td>\n",
       "      <td>From: hambidge@bms.com\\nSubject: Re: Gun Contr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>Pakistan on revenge mission\\n\\nPakistan's cric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>politics</td>\n",
       "      <td>From: garrett@Ingres.COM\\nSubject: Re: Return ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                                            Article\n",
       "0  politics  From: thomasr@cpqhou.se.hou.compaq.com (G. Tho...\n",
       "1  politics  ukip s secret weapon  by any measure  new york...\n",
       "2  politics  From: hambidge@bms.com\\nSubject: Re: Gun Contr...\n",
       "3    sports  Pakistan on revenge mission\\n\\nPakistan's cric...\n",
       "4  politics  From: garrett@Ingres.COM\\nSubject: Re: Return ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(\"dataset/Dataset0.xlsx\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "483f40036cdc496a43796b88046874c12c19cb76",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "colab_type": "code",
    "id": "CUw2hqtv7vZK",
    "outputId": "5939ed6a-efed-4c0c-c697-e77815161198"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "politics          3626\n",
       "Food & drink      2543\n",
       "Technology        2134\n",
       "sports            1963\n",
       "Promotion          876\n",
       "entertainment      541\n",
       "Family             297\n",
       "travel             297\n",
       "education          133\n",
       "style & beauty      74\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "58e07108e5cb00de0e0a274b2225aa8834c43393",
    "colab_type": "text",
    "id": "5u6HXfm_ciaR"
   },
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Raw text data will be transformed into feature vectors and new features will be created using the existing dataset. \n",
    "\n",
    "1. Tokenizing\n",
    "2. Word Embeddings as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ocQ8tsMxjv-k"
   },
   "source": [
    "### Train Test Spliting\n",
    "spliting the dataset into training and validation sets so that we can train and test classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "puDt9PGvkbMK"
   },
   "outputs": [],
   "source": [
    "np.random.seed(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NKguHltOjv-m"
   },
   "outputs": [],
   "source": [
    "train_X, valid_X, train_Y, valid_Y = model_selection.train_test_split(data['Article'].values, data['Category'].values, shuffle = False, test_size = 0.5, random_state = 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CX_bitwLnoIS"
   },
   "outputs": [],
   "source": [
    "valid_X, test_X, valid_Y,test_Y = model_selection.train_test_split(valid_X, valid_Y, shuffle = False, test_size = 0.5, random_state = 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QYo-Wwf5jv-p"
   },
   "source": [
    "### Tokenizing\n",
    "\n",
    "Turning text into vectorize numbers or sequence of intergers, each integer being the index of a token in a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commands:\n",
    "1. text.Tokenizer()\n",
    "        a. Keras provides a more sophisticated API for preparing text that can be fit and reused to prepare multiple \n",
    "        text documents. This may be the preferred approach for large projects.\n",
    "        b. Keras provides the Tokenizer class for preparing text documents for deep learning. The Tokenizer must be \n",
    "        constructed and then fit on either raw text documents or integer encoded text documents.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. tokenizer.texts_to_sequences   > Text line seperated words to numbers\n",
    "    \n",
    "        By default, this function automatically does 3 things:\n",
    "\n",
    "        a. Splits words by space (split=” “).\n",
    "        b. Filters out punctuation (filters=’!”#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n’).\n",
    "        c. Converts text to lowercase (lower=True).\n",
    "        d. Changing to number based on fit tokenizer data. \n",
    "\n",
    "3. sequence.pad_sequences(maxlen=5000) \n",
    "   \n",
    "        a. This function transforms a list of 'num_samples' sequences (lists of integers) into a 2D Numpy array of shape \n",
    "        '(num_samples, num_timesteps)'. 'num_timesteps' is either the 'maxlen' argument if provided, or the length of the \n",
    "        longest sequence otherwise. Sequences that are shorter than 'num_timesteps' or 'maxlen' are padded with zero at \n",
    "        the end.\n",
    "        Sequences longer than 'num_timesteps' or 'maxlen' are shortened so that they fit the desired length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O3pSZVdwjv-r"
   },
   "outputs": [],
   "source": [
    "# create a tokenizer \n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(data['Article'])# fit the tokenizer on the documents\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "icVEavUJkf5P"
   },
   "outputs": [],
   "source": [
    "# convert text to sequence of tokens and pad them to ensure equal length vectors\n",
    "train_x = sequence.pad_sequences(tokenizer.texts_to_sequences(train_X),maxlen=5000)\n",
    "valid_x = sequence.pad_sequences(tokenizer.texts_to_sequences(valid_X),maxlen=5000)\n",
    "test_x = sequence.pad_sequences(tokenizer.texts_to_sequences(test_X),maxlen=5000)\n",
    "# article_data = sequence.pad_sequences(tokenizer.texts_to_sequences(data['Article']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bsKpuZKOjv-t"
   },
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W4uaaKb0jv-w"
   },
   "source": [
    "A word embedding is a form of representing words and documents using a dense vector representation. The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used. Word embeddings can be trained using the input corpus itself or can be generated using pre-trained word embeddings such as Glove, FastText, and Word2Vec. We have use Word2Vec pretrained moel.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Loading the pretrained word embeddings\n",
    "2. Transforming text documents to sequence of tokens and pad them\n",
    "3. Create a mapping of token and their respective embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y-8VHb6wknME"
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "M1pKK1Gqjv-y",
    "outputId": "d6a393f3-8c32-406e-8286-fe8fd4c8aeb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load the pre-trained word-embedding vectors. these weights trained on alarge numbers of words.\n",
    "for i, line in enumerate(open('Model/wiki-news-300d-1M.vec', encoding=\"utf8\")):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JMZ4kCA6kkLL"
   },
   "outputs": [],
   "source": [
    "# create token-embedding mapping\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AqsEANnIjv-0"
   },
   "source": [
    "### Categorical Veriable Handling\n",
    "\n",
    "As our target values consists of names of Categories, so we need to convert them numbers to use it in model. For it we use label encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bd2j-T-qjv-1"
   },
   "outputs": [],
   "source": [
    "# Use sklearn utility to convert label strings to numbered index\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(data['Category'])\n",
    "train_y0 = encoder.transform(train_Y)\n",
    "valid_y0 = encoder.transform(valid_Y)\n",
    "test_y0 = encoder.transform(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y1Rvav3bkuaI"
   },
   "outputs": [],
   "source": [
    "num_classes = np.max(train_y0) + 1\n",
    "train_y = to_categorical(train_y0, num_classes)\n",
    "valid_y = to_categorical(valid_y0, num_classes)\n",
    "test_y = to_categorical(test_y0, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B3YcBjaHjv-3"
   },
   "source": [
    "## Model Building\n",
    "\n",
    "The final step in the text classification framework is to train a classifier using the features created in the previous step. There are many different choices of machine learning models which can be used to train a final model. We will use CNN to learn the deep features of text.\n",
    "\n",
    "In Convolutional neural networks, convolutions over the input layer are used to compute the output. This results in local connections, where each region of the input is connected to a neuron in the output. Each layer applies different filters and combines their results.\n",
    "\n",
    "![model](model.png)\n",
    "\n",
    "\n",
    "Layers Functions:\n",
    "1. layers.Input(features_numbers,):\n",
    "    Creating a Tensor point that will consist of number of features as input. \n",
    "\n",
    "\n",
    "2. embedding_layer = layers.Embedding():\n",
    "    embedding layer comes up with a relation of the inputs in another dimension.\n",
    "        a. Basically, our neural network captures underlying structure of the inputs (our sentences) and puts relation  \n",
    "        between words in our vocabulary into a higher dimension (let's say 2) by optimization.\n",
    "        b. Deeper understanding would say that the frequency of each word appearing with another word from our vocabulary\n",
    "        influences (in a very naive approach we can calculate it by hand)\n",
    "        c. Aforementioned frequency could be one of many underlying structures that NN can capture\n",
    "        d. You can find the intuition on the [youtube link](https://www.youtube.com/watch?v=kw9R0nD69OU) explaining the\n",
    "        word embeddings \n",
    "        e.Word-embedding techniques such as word2vec try to capture the full meaning of words in the resulting embedding, \n",
    "        the embedding layer in a supervised network might not learn such a semantically-rich and general representation.It \n",
    "        is often useful to initialize your embedding layer with weights learned by word2vec on a big corpus.\n",
    "\n",
    "\n",
    "3. layers.SpatialDropout1D():\n",
    "    This version performs the same function as Dropout, however it drops entire 1D feature maps instead of ndividual \n",
    "    elements. As our Data is 2D array so it delete the of specific words randomely during training.\n",
    "\n",
    "\n",
    "4. layers.Convolution1D():\n",
    "    This layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) \n",
    "    dimension to produce a tensor of outputs.\n",
    "    \n",
    "\n",
    "5. layers.GlobalMaxPool1D():\n",
    "    Perform Max pooling operation on the data as 1D. Means Taking the max entries in each row and forward it to next\n",
    "\n",
    "\n",
    "6. layers.Dense(): Pass the data to densily connected Neurons.\n",
    "\n",
    "\n",
    "7. layers.Dropout(): Dropout the percentage of Neurons Randomely during training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e4_b9sxbjv-4"
   },
   "outputs": [],
   "source": [
    "def create_cnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((train_x.shape[1], ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "#   embedding_layer=layers.add(embedding_layer)\n",
    "    \n",
    "    \n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable = False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(num_classes, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9TX2kc7xjv-7"
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 135
    },
    "colab_type": "code",
    "id": "YD89zZMPk3cE",
    "outputId": "d3df76c1-aea4-43ea-b09e-2161124ebd49"
   },
   "outputs": [],
   "source": [
    "classifier = create_cnn() #calling the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "colab_type": "code",
    "id": "T4msphqJjv-7",
    "outputId": "fbf0320d-1a85-48db-ff4a-78dd8553b230",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4993 samples, validate on 1249 samples\n",
      "Epoch 1/5\n",
      "4993/4993 [==============================] - 12s 2ms/step - loss: 1.3822 - val_loss: 0.7753\n",
      "Epoch 2/5\n",
      "4993/4993 [==============================] - 10s 2ms/step - loss: 0.6153 - val_loss: 0.4225\n",
      "Epoch 3/5\n",
      "4993/4993 [==============================] - 10s 2ms/step - loss: 0.3805 - val_loss: 0.3159\n",
      "Epoch 4/5\n",
      "4993/4993 [==============================] - 9s 2ms/step - loss: 0.2755 - val_loss: 0.2670\n",
      "Epoch 5/5\n",
      "4993/4993 [==============================] - 9s 2ms/step - loss: 0.2057 - val_loss: 0.2379\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the classifier\n",
    "hist=classifier.fit(train_x, train_y,epochs=5,verbose=1,shuffle=True,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "0WPpWtjQk1-J",
    "outputId": "7c7be5c6-d727-474c-add9-c9e9c89cf86e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  0.9333546940083307\n"
     ]
    }
   ],
   "source": [
    "# predict the labels on validation dataset\n",
    "predictions = classifier.predict(valid_x)\n",
    "\n",
    "predictions = predictions.argmax(axis=-1) # index of maximum value in an array\n",
    "\n",
    "accuracy=metrics.accuracy_score(predictions, valid_y0)\n",
    "print ('Accuracy on test data: ',  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "agDT1xFEk7q_"
   },
   "outputs": [],
   "source": [
    "#saving the model so that it can be used next time.\n",
    "classifier.save_weights('Model/model_trained_weights.hd5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qF3KCf-2jv--"
   },
   "source": [
    "### Predicting and testing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jaJz_7ljltd7"
   },
   "outputs": [],
   "source": [
    "# Here's how to generate a prediction on individual examples\n",
    "text_labels = encoder.classes_ \n",
    "tested_index=np.random.randint(0,len(test_x),size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1610
    },
    "colab_type": "code",
    "id": "pYozz0yQjv-_",
    "outputId": "dda82fee-e9d8-4e84-f7e1-ae196305073c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2632\n",
      "...\n",
      "From: margoli@watson.ibm.com (Larry Margolis)\n",
      "Subject: Re: I thought commercial Advertising was Not allowed\n",
      "Distribution: na\n",
      "News-Software: IBM OS/2 P\n",
      "...\n",
      "Actual label: politics\n",
      "Predicted label: Technology\n",
      "\n",
      "1296\n",
      "...\n",
      "From: romdas@uclink.berkeley.edu (Ella I Baff)\n",
      "Subject: GETTING AIDS FROM ACUPUNCTURE NEEDLES\n",
      "Organization: University of California, Berkeley\n",
      "Lines: \n",
      "...\n",
      "Actual label: Technology\n",
      "Predicted label: Technology\n",
      "\n",
      "3026\n",
      "...\n",
      "From: aldridge@netcom.com (Jacquelin Aldridge)\n",
      "Subject: Re: Candida(yeast) Bloom, Fact or Fiction\n",
      "Organization: NETCOM On-line Communication Services \n",
      "...\n",
      "Actual label: Technology\n",
      "Predicted label: Technology\n",
      "\n",
      "1505\n",
      "...\n",
      "For Pastry: Sift sugar and flour into bowl and cut in chilled butter. Blend in cold water until it forms a ball and set aside to rest.\n",
      "Preheat oven to\n",
      "...\n",
      "Actual label: Food & drink\n",
      "Predicted label: Food & drink\n",
      "\n",
      "334\n",
      "...\n",
      "From: turpin@cs.utexas.edu (Russell Turpin)\n",
      "Subject: Re: Science and methodology (was: Homeopathy ... tradition?)\n",
      "Organization: CS Dept, University of\n",
      "...\n",
      "Actual label: Technology\n",
      "Predicted label: Technology\n",
      "\n",
      "742\n",
      "...\n",
      "For the parsley pesto: Bring a medium pot of water to a rolling boil. Add the kosher salt until the water tastes like seawater and stir to blend. Add \n",
      "...\n",
      "Actual label: Food & drink\n",
      "Predicted label: Food & drink\n",
      "\n",
      "1356\n",
      "...\n",
      "I'm Endorsing and Volunteering for Bernie! Bernie has been there with us every time, fighting for fairness, for environmental justice, for voting righ\n",
      "...\n",
      "Actual label: politics\n",
      "Predicted label: politics\n",
      "\n",
      "3020\n",
      "...\n",
      "Cut the tenderloin into 8 by 1/2-inch thick slices, 2 per serving. Peel garlic clove. Measure Brandy and warm. Open champignons and drain. Measure cre\n",
      "...\n",
      "Actual label: Food & drink\n",
      "Predicted label: Food & drink\n",
      "\n",
      "2692\n",
      "...\n",
      "How To Pick The Right Seat For 40,000 Feet Have you ever experienced this scenario? You are nearly finished with the online booking for your next flig\n",
      "...\n",
      "Actual label: travel\n",
      "Predicted label: Family\n",
      "\n",
      "339\n",
      "...\n",
      "Subject: good book\n",
      "From: RGINZBERG@eagle.wesleyan.edu (Ruth Ginzberg)\n",
      "Distribution: world\n",
      "Organization: Philosophy Dept., Wesleyan University\n",
      "Nntp-Pos\n",
      "...\n",
      "Actual label: Technology\n",
      "Predicted label: Technology\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tested_index:\n",
    "    prediction = classifier.predict(np.array([test_x[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction)]\n",
    "    print(i)\n",
    "    print(\"...\" + \"\\n\" + test_X[i][:150] + \"\\n\" \"...\")\n",
    "    print(\"Actual label: \" + text_labels[test_y0[i]])\n",
    "    print(\"Predicted label: \" + predicted_label + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GuZsVfNnjv_E"
   },
   "source": [
    "## Testing on New Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9sjnhPdhjv_F"
   },
   "outputs": [],
   "source": [
    "classifier.load_weights('Model/model_trained_weights.hd5') # Loading the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "mlFgDk8kjv_K",
    "outputId": "297d325c-f49d-4ac5-a825-e1a5c47af6ad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>politics</td>\n",
       "      <td>From: thomasr@cpqhou.se.hou.compaq.com (G. Tho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>politics</td>\n",
       "      <td>ukip s secret weapon  by any measure  new york...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>politics</td>\n",
       "      <td>From: hambidge@bms.com\\nSubject: Re: Gun Contr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>Pakistan on revenge mission\\n\\nPakistan's cric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>politics</td>\n",
       "      <td>From: garrett@Ingres.COM\\nSubject: Re: Return ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                                            Article\n",
       "0  politics  From: thomasr@cpqhou.se.hou.compaq.com (G. Tho...\n",
       "1  politics  ukip s secret weapon  by any measure  new york...\n",
       "2  politics  From: hambidge@bms.com\\nSubject: Re: Gun Contr...\n",
       "3    sports  Pakistan on revenge mission\\n\\nPakistan's cric...\n",
       "4  politics  From: garrett@Ingres.COM\\nSubject: Re: Return ..."
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test_data=pd.read_excel('dataset/Dataset0.xlsx')\n",
    "new_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UCX49hv1jv_S"
   },
   "outputs": [],
   "source": [
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "new_article_data = sequence.pad_sequences(tokenizer.texts_to_sequences(new_test_data['Article']),maxlen=train_x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fd85T5Ufjv_Y",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction=classifier.predict(new_article_data)##predicting on new data\n",
    "predicted_label = text_labels[np.argmax(prediction,axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "K9in7LUdlXsX",
    "outputId": "3cc93ee1-790f-4b48-d967-ec711806fdf6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Article</th>\n",
       "      <th>predicted labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>politics</td>\n",
       "      <td>From: thomasr@cpqhou.se.hou.compaq.com (G. Tho...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>politics</td>\n",
       "      <td>ukip s secret weapon  by any measure  new york...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>politics</td>\n",
       "      <td>From: hambidge@bms.com\\nSubject: Re: Gun Contr...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>Pakistan on revenge mission\\n\\nPakistan's cric...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>politics</td>\n",
       "      <td>From: garrett@Ingres.COM\\nSubject: Re: Return ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                                            Article predicted labels\n",
       "0  politics  From: thomasr@cpqhou.se.hou.compaq.com (G. Tho...         politics\n",
       "1  politics  ukip s secret weapon  by any measure  new york...         politics\n",
       "2  politics  From: hambidge@bms.com\\nSubject: Re: Gun Contr...         politics\n",
       "3    sports  Pakistan on revenge mission\\n\\nPakistan's cric...           sports\n",
       "4  politics  From: garrett@Ingres.COM\\nSubject: Re: Return ...         politics"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test_data['predicted labels']=predicted_label ## saving the result on dataframe\n",
    "new_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZEsZ6Rj5qSYe"
   },
   "outputs": [],
   "source": [
    "new_test_data.to_excel('Result/result_data.xlsx',engine='xlsxwriter') ##saving the excel file.\n",
    "plot_model(classifier, to_file='Model/model.png', show_shapes=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Cnn Text classifier revised (Final).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
